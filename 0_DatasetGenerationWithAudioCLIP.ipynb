{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_DatasetGenerationWithAudioCLIP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cd-4R0EUVyZw"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "848a42cb714a46269bda8ea69d55b8af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e64a5db1ba4495a91a8b5b409d0d15f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3cb9d027a7bc4efcb6307d52eefa51fa",
              "IPY_MODEL_1208ab6271b248cd92ba37154737c842",
              "IPY_MODEL_2c22533e8066496f93b7415041e8d3e3"
            ]
          }
        },
        "3e64a5db1ba4495a91a8b5b409d0d15f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3cb9d027a7bc4efcb6307d52eefa51fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c03c8e50630e4fa493d61b5097cbda16",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e60da5b3d7944e781d461c514c0cf02"
          }
        },
        "1208ab6271b248cd92ba37154737c842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce943de3a8244b4680ae3dd7cfc9380b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10d8f1a973fc4625ac4ba7d0725795ec"
          }
        },
        "2c22533e8066496f93b7415041e8d3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ce3bc9e08b14dafb9ecc19a188a40e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:19&lt;00:00, 44.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01508b5d64a549b4911353e7a8bc6ddf"
          }
        },
        "c03c8e50630e4fa493d61b5097cbda16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e60da5b3d7944e781d461c514c0cf02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce943de3a8244b4680ae3dd7cfc9380b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10d8f1a973fc4625ac4ba7d0725795ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ce3bc9e08b14dafb9ecc19a188a40e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01508b5d64a549b4911353e7a8bc6ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yhPW_R2V_kx",
        "outputId": "88ef1d0f-18c9-4f0f-84ad-5abef6a7e19d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 20 17:01:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2WVIYP3WdJG"
      },
      "source": [
        "# AudioCLIP + VQGAN\n",
        "Authored by [Daniel Russell](https://twitter.com/danielrussruss)\n",
        "\n",
        "AudioCLIP: https://github.com/AndreyGuzhov/AudioCLIP\n",
        "\n",
        "VQGAN: https://github.com/CompVis/taming-transformers\n",
        "\n",
        "Inspired by work from [Ryan Murdock](https://twitter.com/advadnoun) and [Katherine Crowson](https://twitter.com/RiversHaveWings). Also, the kornia augmentations are loosely based on work from [jbustter](https://twitter.com/jbusted1) and [BoneAmputee](https://twitter.com/RiversHaveWings).\n",
        "\n",
        "Special thanks to [Anton Wiehe](https://twitter.com/AntonWiehe) for help with importing AudioCLIP's audio encoder properly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd-4R0EUVyZw"
      },
      "source": [
        "# AudioCLIP Imports, Installs and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VVy-WYGYSNQ7",
        "outputId": "c629419e-1c4b-409e-c70b-a36deded9324"
      },
      "source": [
        "!pip install taming-transformers kornia pytorch-ignite visdom ftfy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting taming-transformers\n",
            "  Downloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▏                        | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 20 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 30 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting kornia\n",
            "  Downloading kornia-0.5.11-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 17.5 MB/s \n",
            "\u001b[?25hCollecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.7-py3-none-any.whl (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 79.3 MB/s \n",
            "\u001b[?25hCollecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (0.10.0+cu111)\n",
            "Collecting omegaconf>=2.0.0\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from taming-transformers) (1.9.0+cu111)\n",
            "Collecting pytorch-lightning>=1.0.8\n",
            "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
            "\u001b[K     |████████████████████████████████| 925 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 79.1 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8->taming-transformers) (2.6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8->taming-transformers) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.0.8->taming-transformers) (3.7.4.3)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 79.6 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 77.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning>=1.0.8->taming-transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (1.41.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (1.35.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (3.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.3.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 79.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 76.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.8->taming-transformers) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.8->taming-transformers) (3.6.0)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.1-py2.py3-none-any.whl (7.4 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, future, visdom, ftfy, torchfile\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=c9db833962d3da4d89b04554c58aa6c18f95966182f1cb37aebfc7602e2b536d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=8295d3c30758f8565ca952cdd6a48b80ccd03301b50f815d9b51f1ccb0011215\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=839854fdad9136608920ee1c4fd77d720bfb2c9213dbd76ecfa3c806c43347dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=9471b156fbc8cdbb415b20659a4b7d09e18246cded057b57bb8465d9c62a4340\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=41343ce45be1696aae4af0223e0f983a162b77c49b422e8ecf3bd28100ddd56a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built antlr4-python3-runtime future visdom ftfy torchfile\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, jsonpointer, future, antlr4-python3-runtime, websocket-client, torchfile, pytorch-lightning, omegaconf, jsonpatch, visdom, taming-transformers, pytorch-ignite, kornia, ftfy\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.7.4.post0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 fsspec-2021.10.1 ftfy-6.0.3 future-0.18.2 jsonpatch-1.32 jsonpointer-2.1 kornia-0.5.11 multidict-5.2.0 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-ignite-0.4.7 pytorch-lightning-1.4.9 taming-transformers-0.0.1 torchfile-0.1.0 torchmetrics-0.5.1 visdom-0.1.8.9 websocket-client-1.2.1 yarl-1.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ywc7OMoSGYU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from datetime import datetime\n",
        "from omegaconf import OmegaConf\n",
        "import sys\n",
        "from taming.models.vqgan import VQModel\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import imageio\n",
        "import PIL\n",
        "import gc\n",
        "from IPython import display\n",
        "import kornia.augmentation as K\n",
        "import os\n",
        "DEVICE = torch.device('cuda:0')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cWJ9w1YSJoe",
        "outputId": "ce1dd1f1-0c19-48f1-c813-03dde64dfda9"
      },
      "source": [
        "# !rm -rf AudioCLIP\n",
        "!git clone https://github.com/russelldc/AudioCLIP\n",
        "sys.path.append('./AudioCLIP')\n",
        "from audioclip import AudioCLIP"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AudioCLIP'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 122 (delta 34), reused 99 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (122/122), 16.77 MiB | 18.97 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/html5lib/_trie/_base.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRCsqSYHSiE6",
        "outputId": "6dcdf810-c046-4965-dbc2-51a30c5079ab"
      },
      "source": [
        "torch.set_grad_enabled(False)\n",
        "\n",
        "!wget --continue https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Partial-Training.pt\n",
        "perceptor = AudioCLIP(pretrained='AudioCLIP-Partial-Training.pt').cuda()\n",
        "perceptor.eval()\n",
        "perceptor_size = 224\n",
        "\n",
        "torch.set_grad_enabled(True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 17:02:45--  https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Partial-Training.pt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/379928229/ca663500-d8dd-11eb-97c3-01018fe9b42b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211020%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211020T170245Z&X-Amz-Expires=300&X-Amz-Signature=fb487438211bb1c1bb354b86f50d9a321a7b28d319d742cfde89118aa0aebbd6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=379928229&response-content-disposition=attachment%3B%20filename%3DAudioCLIP-Partial-Training.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-10-20 17:02:45--  https://github-releases.githubusercontent.com/379928229/ca663500-d8dd-11eb-97c3-01018fe9b42b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211020%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211020T170245Z&X-Amz-Expires=300&X-Amz-Signature=fb487438211bb1c1bb354b86f50d9a321a7b28d319d742cfde89118aa0aebbd6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=379928229&response-content-disposition=attachment%3B%20filename%3DAudioCLIP-Partial-Training.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 537302068 (512M) [application/octet-stream]\n",
            "Saving to: ‘AudioCLIP-Partial-Training.pt’\n",
            "\n",
            "AudioCLIP-Partial-T 100%[===================>] 512.41M  61.6MB/s    in 9.6s    \n",
            "\n",
            "2021-10-20 17:02:55 (53.3 MB/s) - ‘AudioCLIP-Partial-Training.pt’ saved [537302068/537302068]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9714f8aed0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lPfuPp8VYoz"
      },
      "source": [
        "class Pars(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pars, self).__init__()\n",
        "        \n",
        "        if init_image:\n",
        "            x = (F.interpolate(torch.tensor(imageio.imread(init_image_path)).unsqueeze(0).permute(0, 3, 1, 2), (sideX, sideY)) / 255).cuda()\n",
        "            z, _, [_, _, indices] = vqgan_model.encode(x)\n",
        "            self.normu = torch.nn.Parameter(z.cuda().clone())\n",
        "        elif blocky_random:\n",
        "            if grayscale_random:\n",
        "                x = torch.zeros(1, 1, random_size, random_size, device=DEVICE).normal_(mean=.3, std=.7).clamp(0, 1).expand(-1, 3, -1, -1)\n",
        "            else:\n",
        "                x = torch.rand(1, 3, random_size, random_size, device=DEVICE).normal_(mean=.3, std=.7).clamp(0, 1).expand(-1, 3, -1, -1)\n",
        "\n",
        "            x = T.Resize((sideX, sideY))(x)\n",
        "            z, _, [_, _, indices] = vqgan_model.encode(x)\n",
        "            self.normu = torch.nn.Parameter(z.cuda().clone())\n",
        "        else:\n",
        "            normu = torch.randn(1, 256, sideX//16, sideY//16, device=DEVICE)\n",
        "            self.normu = torch.nn.Parameter(torch.sinh(1.9 * torch.arcsinh(normu)))\n",
        "\n",
        "    def forward(self):\n",
        "        return vqgan_model.decode(self.normu)\n",
        "\n",
        "nom = T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "augs = torch.nn.Sequential(\n",
        "#     K.RandomHorizontalFlip(p=0.5),\n",
        "    K.RandomAffine(degrees=25, translate=0.1, p=0.8, padding_mode='border'),\n",
        "    K.RandomErasing(p=0.1),\n",
        "    K.RandomPerspective(distortion_scale=0.7, p=0.7),\n",
        ").cuda()\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "def diff_relu(x):\n",
        "    return (torch.sqrt(x*x+0.0001)+x)*0.5\n",
        "def diff_clamp(x):\n",
        "    return diff_relu(1-diff_relu(1-x))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "848a42cb714a46269bda8ea69d55b8af",
            "3e64a5db1ba4495a91a8b5b409d0d15f",
            "3cb9d027a7bc4efcb6307d52eefa51fa",
            "1208ab6271b248cd92ba37154737c842",
            "2c22533e8066496f93b7415041e8d3e3",
            "c03c8e50630e4fa493d61b5097cbda16",
            "4e60da5b3d7944e781d461c514c0cf02",
            "ce943de3a8244b4680ae3dd7cfc9380b",
            "10d8f1a973fc4625ac4ba7d0725795ec",
            "3ce3bc9e08b14dafb9ecc19a188a40e5",
            "01508b5d64a549b4911353e7a8bc6ddf"
          ]
        },
        "id": "T_lHfYj0VFxh",
        "outputId": "37612e62-3d6d-4706-ba00-453b4b4e89af"
      },
      "source": [
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "!wget --continue 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'imagenet_16384.ckpt' \n",
        "!wget --continue 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'imagenet_16384.yaml' \n",
        "\n",
        "vqgan_model = load_vqgan_model('imagenet_16384.yaml', 'imagenet_16384.ckpt').to(DEVICE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 17:03:10--  https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/4d9616e8-7b60-46b0-8daa-8220cf4495bc/last.ckpt [following]\n",
            "--2021-10-20 17:03:10--  https://heibox.uni-heidelberg.de/seafhttp/files/4d9616e8-7b60-46b0-8daa-8220cf4495bc/last.ckpt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 980092370 (935M) [application/octet-stream]\n",
            "Saving to: ‘imagenet_16384.ckpt’\n",
            "\n",
            "imagenet_16384.ckpt 100%[===================>] 934.69M  14.9MB/s    in 62s     \n",
            "\n",
            "2021-10-20 17:04:12 (15.1 MB/s) - ‘imagenet_16384.ckpt’ saved [980092370/980092370]\n",
            "\n",
            "--2021-10-20 17:04:12--  https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/eb9924fa-0931-45f2-93ee-60415d6b2c38/model.yaml [following]\n",
            "--2021-10-20 17:04:12--  https://heibox.uni-heidelberg.de/seafhttp/files/eb9924fa-0931-45f2-93ee-60415d6b2c38/model.yaml\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 692 [application/octet-stream]\n",
            "Saving to: ‘imagenet_16384.yaml’\n",
            "\n",
            "imagenet_16384.yaml 100%[===================>]     692  --.-KB/s    in 0.007s  \n",
            "\n",
            "2021-10-20 17:04:12 (93.1 KB/s) - ‘imagenet_16384.yaml’ saved [692/692]\n",
            "\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "848a42cb714a46269bda8ea69d55b8af",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vgg_lpips model from https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1 to taming/modules/autoencoder/lpips/vgg.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8.19kB [00:00, 1.03MB/s]                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored from imagenet_16384.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIhi1rdmWAk6"
      },
      "source": [
        "def augment(into, cutn=32):\n",
        "    sideY, sideX = into.shape[2:4]\n",
        "    max_size = min(sideX, sideY)\n",
        "    min_size = min(sideX, sideY, perceptor_size)\n",
        "    cutouts = []\n",
        "    for ch in range(cutn):\n",
        "        size = int(torch.rand([])**1 * (max_size - min_size) + min_size)\n",
        "        offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "        offsety = torch.randint(0, sideY - size + 1, ())\n",
        "        cutout = into[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "        cutouts.append(F.interpolate(cutout, (perceptor_size, perceptor_size), mode='bilinear', align_corners=True))\n",
        "        del cutout\n",
        "    \n",
        "    cutouts = torch.cat(cutouts, dim=0)\n",
        "    cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "    cutouts = augs(cutouts)\n",
        "    return cutouts\n",
        "\n",
        "def save_image(img, num=0):    \n",
        "    pil_img = PIL.ImageOps.autocontrast(img)\n",
        "\n",
        "    # Save individual image with timestamp\n",
        "    if itt % display_rate == 0:\n",
        "        current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
        "        img_filename = f'{out_folder}/audioclip_output{str(num)}_{current_time}.jpg'\n",
        "        pil_img.save(img_filename, quality=95, subsampling=0)\n",
        "\n",
        "def checkin():\n",
        "    with torch.no_grad():\n",
        "        img_out = clamp_with_grad(lats(), 0, 1)\n",
        "\n",
        "        batch_num = 0\n",
        "        for img in img_out:\n",
        "          pil_img = T.ToPILImage()(img.squeeze())\n",
        "          save_image(pil_img, batch_num)\n",
        "          batch_num += 1\n",
        "            \n",
        "        if itt % display_rate == 0:\n",
        "            display.clear_output(wait=True)\n",
        "            display.display(pil_img)\n",
        "\n",
        "def ascend_txt():\n",
        "    into = augment(lats(), sample_cuts)\n",
        "    img_enc = perceptor.encode_image(nom(into))\n",
        "    return -main_weight * torch.cosine_similarity(audio_enc, img_enc, -1).mean()\n",
        "\n",
        "def train():\n",
        "    loss = ascend_txt()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if anneal_lr:\n",
        "        optimizer.param_groups[0]['lr'] = max(optimizer.param_groups[0]['lr'] * .995, min_learning_rate)\n",
        "        optimizer.param_groups[0]['weight_decay'] *= .995\n",
        "\n",
        "    checkin()\n",
        "\n",
        "    if itt % 1 == 0:\n",
        "        print('itt', itt, 'loss', loss.detach())\n",
        "        for g in optimizer.param_groups:\n",
        "            print(g['lr'], 'lr', g['weight_decay'], 'decay')\n",
        "\n",
        "def loop(range_val):\n",
        "    global itt\n",
        "    itt = 1\n",
        "\n",
        "    for i in range(range_val):\n",
        "        train()\n",
        "        itt += 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE02CC9DV39f"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25h92C0ZQ2ZL"
      },
      "source": [
        "Here's where we guide CLIP with both our camel recordings and input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0SLwTOzUWKd"
      },
      "source": [
        "sideY, sideX = [512, 512]\n",
        "\n",
        "audio_path = '/content/drive/MyDrive/aiartaton2.0/testing/camel-audio/lament-cut.wav'\n",
        "audio_enc = perceptor.create_audio_encoding(audio_path)\n",
        "audio_enc = audio_enc / audio_enc.norm(dim=-1, keepdim=True)\n",
        "\n",
        "init_image = True\n",
        "init_image_path = '/content/drive/MyDrive/aiartaton2.0/testing/camel-imgs/1.jpg'"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfzSNKRRKnm"
      },
      "source": [
        "5-10 to runs with these settings should be enough to create a dataset, using different audio sources and input images.\n",
        "\n",
        "This process still has batching/iterating potential to explore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxWTub6pRIVF"
      },
      "source": [
        "display_rate = 25\n",
        "\n",
        "learning_rate = 0.06\n",
        "anneal_lr = True #False\n",
        "min_learning_rate = 0.0001\n",
        "dec = .1 #.0\n",
        "\n",
        "num_iterations = 4000\n",
        "sample_cuts = 32 #32\n",
        "\n",
        "main_weight = 25\n",
        "\n",
        "blocky_random = True #True\n",
        "grayscale_random = False\n",
        "random_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtBrcvYiWXRv"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPPD46BSYZDR",
        "cellView": "form",
        "outputId": "9a418f4a-c7f9-4ad6-a467-c0ead6f06abb"
      },
      "source": [
        "#@title Clear memory\n",
        "optimizer.zero_grad()\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9I7kIaKWUuf"
      },
      "source": [
        "out_folder = '/content/drive/MyDrive/aiartaton2.0/testing/camel-clip-data'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL9Fm5GTYZg"
      },
      "source": [
        "# Restart\n",
        "!rm -rf {out_folder}\n",
        "!mkdir {out_folder}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0f42u4dWWKB",
        "cellView": "form"
      },
      "source": [
        "#@title Run\n",
        "lats = Pars().cuda()\n",
        "optimizer = torch.optim.AdamW(params=[lats.normu], lr=learning_rate, weight_decay=dec)\n",
        "\n",
        "loop(num_iterations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ChVXvGHR4ky"
      },
      "source": [
        "Verify how many images we have in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSv72ImyOOCI",
        "outputId": "35a860a9-6d62-46d4-dc8b-96f6491f7e66"
      },
      "source": [
        "!ls $out_folder | wc -l"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh9EO-JfTgFf"
      },
      "source": [
        "!zip -r /content/camel-clip-data.zip $out_folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMGl_S_NUtgL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}